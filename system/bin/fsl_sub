#!/bin/sh

# fsl_sub

# Copyright (C) 2007-2014 University of Oxford
# Authors: Dave Flitney, Stephen Smith, Matthew Webster and Duncan Mortimer
# Sourced from fsl_sub_mgh by Anastasia Yendiki and fsl_sub by FSL staff
# Merged and edited by Matt Allbright

#   Part of FSL - FMRIB's Software Library
#   http://www.fmrib.ox.ac.uk/fsl
#   fsl@fmrib.ox.ac.uk
#
#   Developed at FMRIB (Oxford Centre for Functional Magnetic Resonance
#   Imaging of the Brain), Department of Clinical Neurology, Oxford
#   University, Oxford, UK
#
#
#   LICENCE
#
#   FMRIB Software Library, Release 5.0 (c) 2012, The University of
#   Oxford (the "Software")
#
#   The Software remains the property of the University of Oxford ("the
#   University").
#
#   The Software is distributed "AS IS" under this Licence solely for
#   non-commercial use in the hope that it will be useful, but in order
#   that the University as a charitable foundation protects its assets for
#   the benefit of its educational and research purposes, the University
#   makes clear that no condition is made or to be implied, nor is any
#   warranty given or to be implied, as to the accuracy of the Software,
#   or that it will be suitable for any particular purpose or for use
#   under any specific conditions. Furthermore, the University disclaims
#   all responsibility for the use which is made of the Software. It
#   further disclaims any liability for the outcomes arising from using
#   the Software.
#
#   The Licensee agrees to indemnify the University and hold the
#   University harmless from and against any and all claims, damages and
#   liabilities asserted by third parties (including claims for
#   negligence) which arise directly or indirectly from the use of the
#   Software or the sale of any products based on the Software.
#
#   No part of the Software may be reproduced, modified, transmitted or
#   transferred in any form or by any means, electronic or mechanical,
#   without the express permission of the University. The permission of
#   the University is not required if the said reproduction, modification,
#   transmission or transference is done without financial return, the
#   conditions of this Licence are imposed upon the receiver of the
#   product, and all original and amended source code is included in any
#   transmitted product. You may be held legally responsible for any
#   copyright infringement that is caused or encouraged by your failure to
#   abide by these terms and conditions.
#
#   You are not permitted under this Licence to use this Software
#   commercially. Use for which any financial return is received shall be
#   defined as commercial use, and includes (1) integration of all or part
#   of the source code or the Software into a product for sale or license
#   by or on behalf of Licensee to third parties or (2) use of the
#   Software or any derivative of it for research with the final aim of
#   developing software products for sale or license to a third party or
#   (3) use of the Software or any derivative of it for research with the
#   final aim of developing non-software products for sale or license to a
#   third party, or (4) use of the Software to provide any service to an
#   external organisation for which payment is received. If you are
#   interested in using the Software commercially, please contact Isis
#   Innovation Limited ("Isis"), the technology transfer company of the
#   University, to negotiate a licence. Contact details are:
#   innovation@isis.ox.ac.uk quoting reference DE/9564.
export LC_ALL=C

###########################################################################
# Edit this file in order to setup FSL to use your local compute
# cluster.
###########################################################################

# Domain mail should be sent to if necessary. The complete address will be
# `whoami`@${fslcluster_mailto_domain}
fslcluster_mailto_domain=localhost
# When should the cluster engine send status email (see qsub -m ... docs)
# Don't send email by default; can be overidden by FSLCLUSTER_MAILOPTS
# environment variable
fslcluster_mailopts="${FSLCLUSTER_MAILOPTS:-n}"

###########################################################################
# The following section determines what to do when fsl_sub is called
# by an FSL program. There are several values for the METHOD variable,
# "SGE", "PBS", "SLURM" and "NONE", each of which is determined through
# environment variables defined by clusters.
###########################################################################
METHOD=SGE

# define the default 'qsub' implementation to be used for cluster submission
# This is specifically called for by SGE/PBS and overwritten in the below sections.
qsub_cmd="qsub"

unset module

# No more need to override METHOD determination
if [ -n "$FSLPARALLEL" ] ; then
  if [ "x$SGE_ROOT" = "x" ] ; then
    if [ -f /usr/local/share/sge/default/common/settings.sh ] ; then
      . /usr/local/share/sge/default/common/settings.sh
    elif [ -f /usr/local/sge/default/common/settings.sh ] ; then
      . /usr/local/sge/default/common/settings.sh
    elif [ "$FSLPARALLEL" = "condor" ] ; then # FSLPARALLEL should not be used for this.
      # if condor shall be used, simply switch to Condor's qsub emulation, still uses METHOD set to SGE
      qsub_cmd="condor_qsub"
    elif [ -d /pbs ] && [ -z $PBS_JOBID ] ; then		# For PBS (AY)
      METHOD=PBS
  	  if [ "x$MYPBSQUEUE" = "x" ] ; then
        MYPBSQUEUE=default
      fi
      if [ "x$MYPBSMAXJOBS" = "x" ] ; then
        MYPBSMAXJOBS=40
      fi
      if [ "x$MYPBSWAIT" = "x" ] ; then
        MYPBSWAIT=20
      fi
    elif [ -f /usr/bin/srun ] && [ -z $SLURM_JOB_ID ] ; then	# For SLURM (AY)
      METHOD=SLURM
      qsub_cmd="sbatch"
      if [ "x$MY_SLURM_PARTITION" = "x" ] ; then
        MY_SLURM_PARTITION=standard
      fi
      if [ "x$MY_SLURM_MAX_JOBS" = "x" ] ; then
        MY_SLURM_MAX_JOBS=200
      fi
      if [ "x$MY_SLURM_WAIT" = "x" ] ; then
        MY_SLURM_WAIT=20
      fi
    else
      METHOD=NONE
    fi
  else
    QCONF=`which qconf`
  	if [ "x$QCONF" = "x" ]; then
  		METHOD=NONE
  		echo "Warning: SGE_ROOT environment variable is set but Grid Engine software not found, will run locally" >&2
  	fi
  fi
else
  METHOD=NONE
fi

# stop submitted scripts from submitting jobs themselves
if [ "X$FSLSUBALREADYRUN" = "Xtrue" ] ; then
    METHOD=NONE
    echo "Warning: job on queue attempted to submit parallel jobs - running jobs serially instead" >&2
fi

if [ "X$METHOD" = "XNONE" ]; then
	QCONF=echo
fi
FSLSUBALREADYRUN=true
export FSLSUBALREADYRUN


###########################################################################
# The following auto-decides what cluster queue to use. The calling
# FSL program will probably use the -T option when calling fsl_sub,
# which tells fsl_sub how long (in minutes) the process is expected to
# take (in the case of the -t option, how long each line in the
# supplied file is expected to take). You need to setup the following
# list to map ranges of timings into your cluster queues - it doesn't
# matter how many you setup, that's up to you.
###########################################################################

map_qname ()
{
  #########################################################################
  # The following has been commented out for deprecation under new edits.
  # You can edit the queues below to reflect your cluster.
  #
  # for Debian we can't do the stuff below, because it would be hard
  # to determine how particular queues are meant to be used on any given
  # system. Instead of translating into a queue name we specify proper
  # resource limits, and let SGE decide what queue matches
  # (qsub wants the time limit in seconds)
  # queueCmd="$queueCmd -l h_rt=$(echo "$1 * 60" | bc)"
  #########################################################################

    if [ $1 -le 20 ] ; then
      queue=veryshort.q
    elif [ $1 -le 120 ] ; then
      queue=short.q
    elif [ $1 -le 1440 ] ; then
      queue=long.q
    else
      queue=verylong.q
    fi
    queueCmd=" -q $queue "

    #echo "Estimated time was $1 mins: queue name is $queue"
}


###########################################################################
# Don't change the following (but keep scrolling down!)
###########################################################################

OLD_POSIXLY_CORRECT=${POSIXLY_CORRECT}
POSIXLY_CORRECT=1
export POSIXLY_CORRECT
command=`basename $0`

usage ()
{
  cat <<EOF

$command V1.1 (SCANLab V0.1) - wrapper for job control system such as SGE

Usage: fsl_sub_mgh [options] <command>

$command gzip *.img *.hdr
$command -q short.q gzip *.img *.hdr
$command -a darwin regscript rawdata outputdir ...

  -T <minutes>          Estimated job length in minutes, used to auto-set queue name
  -q <queuename>        Possible values for <queuename> are "verylong.q", "long.q"
                        "short.q" and "veryshort.q". See below for details
                        Default is "long.q".
  -a <arch-name>        Architecture [e.g., darwin or lx24-amd64] or consumable
                        resources from Slurm's gref.conf
  -p <job-priority>     Lower priority [0:-1024] default = 0
  -M <email-address>    Who to email, default = `whoami`@fmrib.ox.ac.uk
  -j <jid>              Place a hold on this task until job jid has completed
  -t <filename>         Specify a task file of commands to execute in parallel
  -N <jobname>          Specify jobname as it will appear on queue
  -l <logdirname>       Where to output logfiles
  -m <mailoptions>      Change the cluster mail options, see qsub or sbatch for
                        details.
  -F                    Use flags embedded in scripts to set SGE queuing options
  -v                    Verbose mode.
  -s <pename>,<threads> Submit a multi-threaded task - requires a PE (<pename>) to be
                        configured for the requested queues.
                        <threads> specifies the number of threads to run
  -S <shell-path>       Change the PBS shell option, see pbsubmit for details

  This is a modified version of FSL's fsl_sub that is compatible with the
  PBS queueing system running on launchpad/seychelles. The above command line
  options work with SGE, but they may not all work with PBS, nor with Slurm.
  This modified version is was first intended to work with bedpostx_mgh, but has
  been edited for more versatility for use in the University of Arizona's SCAN Lab.
EOF

  exit 1
}
# This part of the help is not applicable to our cluster
#
#Queues:
#
#There are several batch queues configured on the cluster, each with defined CPU
#time limits. All queues, except bigmem.q, have a 8GB memory limit.
#
#veryshort.q:This queue is for jobs which last under 30mins.
#short.q:    This queue is for jobs which last up to 4h.
#long.q:     This queue is for jobs which last less than 24h. Jobs run with a
#            nice value of 10.
#verylong.q: This queue is for jobs which will take longer than 24h CPU time.
#            There is one slot per node, and jobs on this queue have a nice value
#            of 15.
#bigmem.q:   This queue is like the verylong.q but has no memory limits.

nargs=$#
if [ $nargs -eq 0 ] ; then
  usage
fi

set -- `getopt T:q:a:p:M:j:t:N:Fvm:l:s:S: $*`
result=$?
if [ $result != 0 ] ; then
  echo "What? Your arguments make no sense!"
fi

if [ $nargs -eq 0 ] || [ $result != 0 ] ; then
  usage
fi

POSIXLY_CORRECT=${OLD_POSIXLY_CORRECT}
export POSIXLY_CORRECT

###########################################################################
# If you have a Parallel Environment configured for OpenMP tasks then
# the variable omp_pe should be set to the name you have defined for that
# PE. The script will work out which queues have that PE setup on them.
# Note, we support openmp tasks even when Grid Engine is not in use.
###########################################################################

omp_pe='openmp'

###########################################################################
# The following sets up the default queue name, which you may want to
# change. It also sets up the basic emailing control.
###########################################################################

if [ "x$FSLCLUSTER_DEFAULT_QUEUE" != "x" ] ; then
    queueCmd=" -q $FSLCLUSTER_DEFAULT_QUEUE "
fi
mailto=`whoami`@${fslcluster_mailto_domain}
MailOpts=${fslcluster_mailopts}

###########################################################################
# In the following, you might want to change the behaviour of some
# flags so that they prepare the right arguments for the actual
# cluster queue submission program, in our case "qsub".
#
# -a sets is the cluster submission flag for controlling the required
# hardware architecture (normally not set by the calling program)
#
# -p set the priority of the job - ignore this if your cluster
# environment doesn't have priority control in this way.
#
# -j tells the cluster not to start this job until cluster job ID $jid
# has completed. You will need this feature.
#
# -t will pass on to the cluster software the name of a text file
# containing a set of commands to run in parallel; one command per
# line.
#
# -N option determines what the command will be called when you list
# running processes.
#
# -l tells the cluster what to call the standard output and standard
# -error logfiles for the submitted program.
###########################################################################

if [ -z $FSLSUBVERBOSE ] ; then
    verbose=0
else
    verbose=$FSLSUBVERBOSE;
    echo "METHOD=$METHOD : args=$@" >&2
fi

scriptmode=0

while [ $1 != -- ] ; do
  case $1 in
    -z)
      if [ -e $2 -o `${FSLDIR}/bin/imtest $2` = 1 ] ; then
        exit 0
      fi
      shift;;
    -T)
      map_qname $2
      if [ $METHOD = PBS ] ; then 	# (AY)
        queue=$MYPBSQUEUE
      fi
      shift;;
    -q)
      queue=$2
      queueCmd=" -q $queue "
      $QCONF -sq $queue >/dev/null 2>&1
      if [ $? -eq 1 ]; then
        echo "Invalid queue specified!"
        exit 127
      fi
      shift;;
    -a)
      sge_arch="-l arch=$2"
      pbs_arch="-l nodes=1:$2"
      slurm_arch="--constraint=$2"
      shift;;
    -p)
      sge_priority="-p $2"
      pbs_priority="-p $2"
      shift;;
    -M)
      mailto=$2
      shift;;
    -j)
      jid=$2
      sge_hold="-hold_jid $jid"
      pbs_hold="-W depend=afterok:$jid"
      slurm_hold="--dependency=afterok:$jid"
      shift;;
    -t)
      taskfile=$2
      if [ -f "$taskfile" ] ; then
        tasks=`wc -l $taskfile | awk '{print $1}'`
        if [ $tasks -ne 0 ]; then
          sge_tasks="-t 1-$tasks"

          # Split jobs in batches so that the total number of jobs running
          # at any given time does not exceed a user-specified maximum (AY)
          if [ $METHOD = PBS ] ; then		# (AY)
            nbatch=$(($tasks/$MYPBSMAXJOBS + ($tasks%$MYPBSMAXJOBS > 0)))
            ninbatch=$(($tasks/$nbatch + ($tasks%$nbatch > 0)))
            pbs_tasks=""
          elif [ $METHOD = SLURM ] ; then		# (AY)
            nbatch=$(($tasks/$MY_SLURM_MAX_JOBS + ($tasks%$MY_SLURM_MAX_JOBS > 0)))
            ninbatch=$(($tasks/$nbatch + ($tasks%$nbatch > 0)))
            slurm_tasks="--array 1-$nbatch"
          fi
        else
          echo "Task file ${taskfile} is empty"
          echo "Should be a text file listing all the commands to run!"
          exit -1
        fi
      else
          echo "Task file (${taskfile}) does not exist"
          exit -1
      fi
      shift;;
    -N)
      JobName=$2;
      shift;;
    -m)
      MailOpts=$2;
      shift;;
    -S)
      pbs_shell="-s $2";
      shift;;
    -l)
    LogOpts="-o $2 -e $2";
    LogDir="${2}/";
    if [ ! -e ${2} ]; then
      mkdir -p $2
    else
      echo ${2} | grep '/dev/null' >/dev/null 2>&1
      if [ $? -eq 1 ] && [ -f ${2} ]; then
        echo "Log destination is a file (should be a folder)"
        exit -1
      fi
    fi
    shift;;
    -F)
      scriptmode=1;
      ;;
    -v)
      verbose=1
      ;;
    -s)
      pe_string=$2;
      peName=`echo $pe_string | cut -d',' -f 1`
      peThreads=`echo $pe_string | cut -d',' -f 2`
      shift;;
  esac
  shift  # next flag
done
shift

###########################################################################
# Don't change the following (but keep scrolling down!)
###########################################################################

if [ -z "$taskfile" ] && [ -z "$1" ]; then
	echo "Either supply a command to run or a parallel task file"
	exit -1
fi

if [ -z "$taskfile" ] && [ ! -x "$1" ]; then
	which $1 >/dev/null 2>&1
	if [ $? -ne 0 ]; then
		echo "The command you have requested cannot be found or is not executable"
		exit -1
	fi
fi

if [ "x$JobName" = x ] ; then
    if [ "x$taskfile" != x ] ; then
	JobName=`basename $taskfile`
    else
	JobName=`basename $1`
    fi
fi

# Deprecated
# if [ "x$tasks" != x ] && [ ! -f "$taskfile" ] ; then
#    echo $taskfile: invalid input!
#    echo Should be a text file listing all the commands to run!
#    exit -1
# fi

if [ "x$tasks" != "x" ] && [ "x$@" != "x" ] ; then
    echo "Spurious input after parsing command line: \"$@\"!"
    echo "You appear to have specified both a task file and a command to run"
    exit -1
fi

if [ "x$peName" != "x" ]; then
    # If the PE name is 'openmp' then limit the number of threads to those specified

    if [ "X$peName" = "X$omp_pe" ]; then
        OMP_NUM_THREADS=$peThreads
	export OMP_NUM_THREADS
    fi
fi

case $METHOD in

###########################################################################
# The following is the main call to the cluster, using the "qsub" SGE
# program. If $tasks has not been set then qsub is running a single
# command, otherwise qsub is processing a text file of parallel
# commands.
###########################################################################

SGE)
   ###########################################################################
   # Test Parallel environment options
   ###########################################################################
if [ "x$peName" != x ]; then
        # Is this a configured PE?

  $QCONF -sp $peName >/dev/null 2>&1

  if [ $? -eq 1 ]; then
    echo $@
    echo "$peName is not a valid PE"
    exit -1
  fi

  if [ "x$queue" = "x" ] ; then
    echo $@
    echo "No queue specified."
    exit -1
  fi
        # Get a list of queues configured for this PE and confirm that the queue
        # we have submitted to has that PE set up.
  qstat -g c -pe $peName >/dev/null 2>&1
  if [ $? -eq 1 ]; then
    echo "No parallel environments configured!"
    exit -1
  fi

  qstat -g c -pe $peName | sed '1,2d' | awk '{ print $1 }' | grep ^$queue >/dev/null 2>&1

  if [ $? -eq 1 ]; then
    echo $@
    echo "PE $peName is not configured on $queue"
    exit -1
  fi

  # The -w e option will result in the job failing if there are insufficient slots
        # on any of the cluster nodes
  pe_options="-pe $peName $peThreads -w e"

fi

if [ "x$tasks" = "x" ] ; then
  if [ $scriptmode -ne 1 ] ; then
    sge_command="$qsub_cmd -V -cwd -shell n -b y -r y $queueCmd $pe_options -M $mailto -N $JobName -m $MailOpts $LogOpts $sge_arch $sge_hold"
  else
    sge_command="$qsub_cmd $LogOpts $sge_arch $sge_hold"
  fi
  if [ $verbose -eq 1 ] ; then
    echo sge_command: $sge_command >&2
    echo executing: $@ >&2
  fi
  exec $sge_command $@ | awk '{print $3}'
  else
    sge_command="$qsub_cmd -V -cwd $queueCmd $pe_options -M $mailto -N $JobName -m $MailOpts $LogOpts $sge_arch $sge_hold $sge_tasks"
    if [ $verbose -eq 1 ] ; then
      echo sge_command: $sge_command >&2
      echo control file: $taskfile >&2
    fi
    exec $sge_command <<EOF | awk '{print $3}' | awk -F. '{print $1}'
#!/bin/sh

#$ -S /bin/sh

command=\`sed -n -e "\${SGE_TASK_ID}p" $taskfile\`

exec /bin/sh -c "\$command"
EOF
fi
;;

###########################################################################
# The following has been modified from "SGE)" above to submit jobs
# to the Martinos Center cluster using PBS. Proceed at own peril. (AY)
###########################################################################

    PBS)
        mailto=`whoami`
	if [ "x$tasks" = "x" ] ; then
	    if [ $scriptmode -ne 1 ] ; then
		qsubopts="-V -r y -q $queue -M $mailto -m $MailOpts"
		qsubopts="$qsubopts -N $JobName"
		qsubopts="$qsubopts $pbs_hold"
		#qsubopts="$qsubopts $LogOpts"
	    else
		#qsubopts="$LogOpts $pbs_hold"
		qsubopts="$pbs_hold"
	    fi
	    cmdline=$@
	    if [ $verbose -eq 1 ] ; then
		echo pbsubmit -o \"$qsubopts\" -c \"$cmdline\" $pbs_arch $pbs_shell >&2
	    fi
	    jobid=`exec pbsubmit -o "$qsubopts" -c "$cmdline" $pbs_arch $pbs_shell | tail -n 1`
	    echo $jobid | awk -v FS=. '{print $1}'
	else
	    if [ "x$LogDir" != x ] ; then
		cp /dev/null $LogDir/joblist
	    fi
	    k=0
	    while [ $k -lt $ninbatch ]; do
		j=$k
		pbs_hold_previous=$pbs_hold
	    	while [ $j -lt $tasks ]; do
		    taskno=`printf '%04d' $j`
		    qsubopts="-V -r y -q $queue -M $mailto -m $MailOpts"
		    qsubopts="$qsubopts -N ${JobName}_$taskno"
		    qsubopts="$qsubopts $pbs_hold_previous"
		    #qsubopts="$qsubopts $LogOpts"
		    let lineno=j+1
		    cmdline=`sed -n -e "$lineno p" $taskfile`
		    if [ $verbose -eq 1 ] ; then
			echo pbsubmit -o \"$qsubopts\" -c \"$cmdline\" $pbs_arch $pbs_shell >&2
		    fi
		    jobid=`exec pbsubmit -o "$qsubopts" -c "$cmdline" $pbs_arch $pbs_shell | tail -n 1`
		    jobid=`echo $jobid | awk -v FS=. '{print $1}'`
		    pbs_hold_previous="-W depend=afterok:$jobid"
		    if [ "x$LogDir" != x ] ; then
			echo ${JobName}_$taskno $jobid >> $LogDir/joblist
		    fi
		    let j=j+ninbatch
		    sleep $MYPBSWAIT
		done
		pbs_hold_last="$pbs_hold_last $jobid"
		let k=k+1
	    done
	    # Return colon-separated list of job IDs:
	    # This will allow the list to be passed as a single argument
	    # to a potential next call of fsl_sub (e.g. $bedpostid in bedpostx).
	    echo $pbs_hold_last | sed -e "s/ /:/g"
	fi
	# Check for jobs that are held and release them:
	# Jobs are held if the jobs they depend on do not exist
	# (e.g., in bedpostx, when a single-slice job happens to
	# be submitted after the preprocessing job has already ended)
	user=`whoami`
	heldjobs=(`qstat -u $user | grep -w H | awk -v FS=. '{print $1}'`)
	if [ ! -z $heldjobs ] ; then
	    qrls ${heldjobs[*]}
        fi
	;;

###########################################################################
# The following has been modified from "SGE)" above to submit jobs
# on a cluster using SLURM. (AY)
###########################################################################

    SLURM)
	if [ "x$tasks" = "x" ] ; then
	    if [ "x$LogDir" = "x" ] ; then
        	slurm_log="--output=$LogDir/slurm-%j.out"
        	slurm_log="$slurm_log --error=$LogDir/slurm-%j.err"
	    fi
	    if [ $scriptmode -ne 1 ] ; then
		      slurm_command="srun --partition=$queue --mail-user=$mailto --job-name=$JobName --mail-type=$MailOpts $slurm_log $slurm_arch $slurm_hold"
	    else
		      slurm_command="srun $slurm_log $slurm_arch $slurm_hold"
	    fi
	    if [ $verbose -eq 1 ] ; then
		      echo slurm_command: $slurm_command >&2
		        echo executing: $@ >&2
	    fi
	    exec $slurm_command $@ | awk '{print $NF}'
    else
	    if [ "x$LogDir" = "x" ] ; then
        	slurm_log="--output=$LogDir/slurm-%A_%a.out"
        	slurm_log="$slurm_log --error=$LogDir/slurm-%A_%a.err"
	    fi
	    slurm_command="sbatch --partition $queue --mail-user $mailto --job-name $JobName --mail-type $MailOpts $slurm_log $slurm_arch $slurm_hold $slurm_tasks"
	    if [ $verbose -eq 1 ] ; then
		      echo slurm_command: $slurm_command >&2
		        echo control file: $taskfile >&2
	    fi
	    exec $slurm_command <<EOF | awk '{print $NF}'
#!/bin/sh

iline=\$(($ninbatch*(\$SLURM_ARRAY_TASK_ID - 1) + 1))
endline=\$((\$iline + $ninbatch))

while [ \$iline -lt \$endline ] && [ \$iline -le $tasks ] ; do
  cmdline=\`sed -n "\$iline p" $taskfile\`
  srun \$cmdline
  let iline=iline+1
  sleep $MY_SLURM_WAIT
done
EOF
	fi
	;;

###########################################################################
# Don't change the following - this runs the commands directly if a
# cluster is not being used.
###########################################################################

NONE)
if [ "x$tasks" = "x" ] ; then
  if [ $verbose -eq 1 ] ; then
    echo executing: $@ >&2
  fi

  /bin/sh <<EOF1 > ${LogDir}${JobName}.o$$ 2> ${LogDir}${JobName}.e$$
$@
EOF1
  ERR=$?
  if [ $ERR -ne 0 ] ; then
    cat ${LogDir}${JobName}.e$$ >&2
    exit $ERR
  fi
else
  if [ $verbose -eq 1 ] ; then
    echo "Running commands in: $taskfile" >&2
  fi

  n=1
  while [ $n -le $tasks ] ; do
    line=`sed -n -e ''${n}'p' $taskfile`
    if [ $verbose -eq 1 ] ; then
      echo executing: $line >&2
    fi
    /bin/sh <<EOF2 > ${LogDir}${JobName}.o$$.$n 2> ${LogDir}${JobName}.e$$.$n
$line
EOF2
    n=`expr $n + 1`
done
  fi
  echo $$
;;

esac

###########################################################################
# Done.
###########################################################################
